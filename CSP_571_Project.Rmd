---
title: "CSP_571_Project"
output:
  pdf_document: default
  html_document: defaul   t
date: "2024-04-1"
---
```{r}

```

## CSP 571 Project

### Data Preparation

#### Importing data

```{r}
library(readr)
crime_data <- read_csv("Crimes_-_2001_to_Present_20231109(1).csv")
head(crime_data)
```
#### Taking a subset of the file for ease of trials
```{r}
# crime_data_2020_to_2023 <- subset(crime_data, Year >= 2020 & Year <= 2023)
# write.csv(crime_data_2020_to_2023, file = "crime_data_2020_to_2023.csv", row.names = FALSE)
```


```{r}
library(readr)
crime_data_subset <- read_csv("crime_data_2020_to_2023.csv")
crime_model_data <- crime_data_subset
str(crime_data_subset)
```
#### Removing irrelevant features
```{r}
library(dplyr)
crime_data_subset <- select(crime_data_subset, Date, `Primary Type`, `Location Description`, District, `Community Area`, Arrest)
str(crime_data_subset)
```
#### Removing duplicates and null values
```{r}
library(dplyr)
crime_data_subset <- distinct(crime_data_subset)
crime_data_subset <- na.omit(crime_data_subset)
str(crime_data_subset)
```
#### Transforming date into seperate columns
```{r}
library(lubridate)
crime_data_subset$Date <- mdy_hms(crime_data_subset$Date)

crime_data_subset$Day <- day(crime_data_subset$Date)
crime_data_subset$Month <- month(crime_data_subset$Date)
crime_data_subset$Year <- year(crime_data_subset$Date)

crime_data_subset$Time <- case_when(
  hour(crime_data_subset$Date) >= 6 & hour(crime_data_subset$Date) < 12 ~ "Morning",
  hour(crime_data_subset$Date) >= 12 & hour(crime_data_subset$Date) < 18 ~ "Afternoon",
  hour(crime_data_subset$Date) >= 18 & hour(crime_data_subset$Date) < 24 ~ "Evening",
  TRUE ~ "Night"
)
crime_data_subset <- select(crime_data_subset, -Date)
head(crime_data_subset)
```
### Data Analysis

#### Getting the top 5 crimes
```{r}
# install.packages("ggplot2")
library(ggplot2)

crime_counts <- crime_data_subset %>%
  count(`Primary Type`, sort = TRUE)

total_crimes <- sum(crime_counts$n)
crime_counts$percentage <- (crime_counts$n / total_crimes) * 100

pie(crime_counts$percentage, labels = paste(crime_counts$`Primary Type`, "\n", round(crime_counts$percentage, 1), "%"), col = rainbow(length(crime_counts$percentage)), main = "Occurrences of Crime", cex = 0.5)

top_5_crimes <- head(crime_counts, 5)

ggplot(top_5_crimes, aes(x = reorder(`Primary Type`, -n), y = n, fill = `Primary Type`)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round((n / total_crimes) * 100, 1), "%")), vjust = -0.5, size = 3) +
  labs(title = "Top 5 Crimes", x = "Crime Type", y = "Occurrences") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = rainbow(length(top_5_crimes$`Primary Type`))) +
  guides(fill = FALSE)
```

#### Getting the data for the top 5 crimes
```{r}
top_5_crimes_data <- crime_data_subset %>%
  filter(`Primary Type` %in% top_5_crimes$`Primary Type`)
str(top_5_crimes_data)
```

#### Analysing the crimes
```{r}
arrest_rate <- top_5_crimes_data %>%
  group_by(`Primary Type`) %>%
  summarise(arrest_rate = mean(Arrest, na.rm = TRUE))

top_5_crimes_with_arrest_rate <- left_join(top_5_crimes, arrest_rate, by = "Primary Type")

arrest_rate_percentage <- top_5_crimes_with_arrest_rate$arrest_rate * 100
pie(arrest_rate_percentage, 
    labels = paste(top_5_crimes_with_arrest_rate$`Primary Type`, "\n", round(arrest_rate_percentage, 2), "%"),
    main = "Arrest Rates for Top 5 Crimes",
    col = rainbow(length(arrest_rate_percentage))
)
```

### Spatial Analysis

##### 1. Distribution of crimes by District and Community Area

```{r}
library(dplyr)
library(ggplot2)

# Define specific crimes to analyze
specific_crimes <- c("BATTERY", "THEFT", "MOTOR VEHICLE THEFT", "CRIMINAL DAMAGE", "ASSAULT")

# Analyze each crime type
for (crime in specific_crimes) {
  cat("\n--- Spatial Analysis for:", crime, "---\n")
  
  crime_data_specific <- crime_data_subset %>% filter(`Primary Type` == crime)
  
  # Distribution by District
  district_counts <- crime_data_specific %>%
    count(District) %>%
    mutate(District = factor(District))  # Ensuring District is treated as a factor for plotting

  plot_districts <- ggplot(district_counts, aes(x = District, y = n, fill = District)) +
    geom_bar(stat = "identity") +
    labs(title = paste("Crime Distribution by District for", crime), x = "District", y = "Count") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

  print(plot_districts)
}

```
##### 2. Arrest Rates by district and Community Area

```{r}
for (crime in specific_crimes) {
  cat("\n--- Spatial Analysis of Arrest Rates for:", crime, "---\n")
  
  crime_data_specific <- crime_data_subset %>% filter(`Primary Type` == crime)
  
  # Arrest Rates by District
  arrest_rate_district <- crime_data_specific %>%
    group_by(District) %>%
    summarise(Arrests = sum(Arrest, na.rm = TRUE),
              Total = n(),
              Arrest_Rate = (Arrests / Total) * 100) %>%
    ungroup()

  plot_arrest_district <- ggplot(arrest_rate_district, aes(x = reorder(District, -Arrest_Rate), y = Arrest_Rate, fill = District)) +
    geom_col() +
    labs(title = paste("Arrest Rate by District for", crime), x = "District", y = "Arrest Rate (%)") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

  print(plot_arrest_district)
}
```
### Temporal Analysis

##### 1. Crime Trend Over Years

Code for Yearly Crime Trends:

```{r}
library(dplyr)
library(ggplot2)

# Analyze crime trends over years
yearly_trends <- crime_data_subset %>%
  count(Year, `Primary Type`, name = "Count") %>%
  filter(`Primary Type` %in% specific_crimes)

ggplot(yearly_trends, aes(x = Year, y = Count, color = `Primary Type`, group = `Primary Type`)) +
  geom_line() +
  geom_point() +
  labs(title = "Yearly Crime Trends by Crime Type", x = "Year", y = "Number of Crimes") +
  theme_minimal()
```


###### 2. Monthly Crime Distribution

Code for Monthly Crime Distribution:

```{r}
# Monthly distribution of crimes
monthly_distribution <- crime_data_subset %>%
  count(Month, `Primary Type`, name = "Count") %>%
  filter(`Primary Type` %in% specific_crimes)

ggplot(monthly_distribution, aes(x = Month, y = Count, fill = `Primary Type`)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Monthly Distribution of Crimes by Type", x = "Month", y = "Number of Crimes") +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +
  theme_minimal()
```


### Model Selection 
```{r}
# Load necessary libraries
library(caret)
library(corrplot)

# Convert categorical variables to factors
crime_data_subset$`Primary Type` <- as.factor(crime_data_subset$`Primary Type`)
crime_data_subset$`Location Description` <- as.factor(crime_data_subset$`Location Description`)
crime_data_subset$District <- as.factor(crime_data_subset$District)
crime_data_subset$`Community Area` <- as.factor(crime_data_subset$`Community Area`)
crime_data_subset$Arrest <- as.factor(crime_data_subset$Arrest)
crime_data_subset$Time <- as.factor(crime_data_subset$Time)

# Correlation Matrix/HeatMap
numeric_cols <- sapply(crime_data_subset, is.numeric)
correlation_matrix <- cor(crime_data_subset[, numeric_cols])
corrplot(correlation_matrix, method="circle")

# PCA
pca <- prcomp(crime_data_subset[, numeric_cols], center = TRUE, scale. = TRUE)
summary(pca)
```
```{r}
head(crime_model_data)
```
```{r}
rf_pred <- predict(rf_model, crimeTest)
print(head(rf_pred))
```

#### Predictive analysis using Random Forest
```{r}
# Load necessary libraries
library(randomForest)
library(class)
library(caret)
library(gmodels)

crime_model_data <- save_data

# Drop unnecessary columns
crime_model_data$ID <- NULL
crime_model_data$`Case Number` <- NULL
crime_model_data$Block <- NULL
crime_model_data$IUCR <- NULL
crime_model_data$Description <- NULL
crime_model_data$`Location Description` <- NULL
crime_model_data$Beat <- NULL
crime_model_data$District <- NULL
crime_model_data$Ward <- NULL
crime_model_data$`Community Area` <- NULL
crime_model_data$`FBI Code` <- NULL
crime_model_data$`X Coordinate` <- NULL
crime_model_data$`Y Coordinate` <- NULL
crime_model_data$`Updated On` <- NULL
crime_model_data$Location <- NULL

# Remove rows with missing values
crime_model_data <- na.omit(crime_model_data)

# Remove columns with all missing values
crime_model_data <- crime_model_data[, colSums(is.na(crime_model_data)) != nrow(crime_model_data)]

# Identify classes with only one record
class_counts <- table(crime_model_data$`Primary Type`)
single_record_classes <- names(class_counts[class_counts == 1])

# Remove these classes
for (class in single_record_classes) {
  crime_model_data <- crime_model_data[!(crime_model_data$`Primary Type` == class),]
}

# Convert categorical variables into factors
crime_model_data$Arrest <- as.factor(crime_model_data$Arrest)
crime_model_data$Domestic <- as.factor(crime_model_data$Domestic)
crime_model_data$Year <- as.factor(crime_model_data$Year)

# Convert 'Primary Type' to a factor
crime_model_data$`Primary Type` <- as.factor(crime_model_data$`Primary Type`)

# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(crime_model_data$`Primary Type`, p = .7, 
                                  list = TRUE, 
                                  times = 1)
crimeTrain <- crime_model_data[ trainIndex$Resample1,]
crimeTest  <- crime_model_data[-trainIndex$Resample1,]

# Check if the saved model file exists
if (file.exists("rf_model.rds")) {
  # Load the model
  rf_model <- readRDS(file = "rf_model.rds")
} else {
  # Train the Random Forest model
  set.seed(123)
  rf_model <- randomForest(`Primary Type` ~ ., data = crimeTrain, ntree = 100)
  
  # Save the model
  saveRDS(rf_model, file = "rf_model.rds")
}

# Evaluate the model
rf_pred <- predict(rf_model, crimeTest)

# Ensure that both rf_pred and crimeTest$`Primary Type` have the same levels
rf_pred <- factor(rf_pred, levels = levels(crimeTest$`Primary Type`))

# Create a cross table of the predicted and actual values
CrossTable(x = rf_pred, y = crimeTest$`Primary Type`,
           prop.chisq = FALSE, # Do not compute chi-squared test
           prop.t = FALSE, # Do not compute t-test
           prop.r = FALSE, # Do not compute row proportions
           dnn = c('predicted', 'actual')) # Set the names of the dimensions
```

#### Predictive analysis using KNN

```{r}
# Load necessary library
library(class)

# Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

crimeTrain_norm <- as.data.frame(lapply(crimeTrain[,sapply(crimeTrain, is.numeric)], normalize))
crimeTest_norm <- as.data.frame(lapply(crimeTest[,sapply(crimeTest, is.numeric)], normalize))

# Prepare training and test set
train_labels <- crimeTrain$`Primary Type`
test_labels <- crimeTest$`Primary Type`

# Check if the saved model file exists
if (file.exists("knn_model.rds")) {
  # Load the model
  knn_pred <- readRDS(file = "knn_model.rds")
} else {
  # Perform KNN
set.seed(123)
knn_pred <- knn(train = crimeTrain_norm, test = crimeTest_norm, cl = train_labels, k=3)
  
}

# Ensure that both knn_pred and test_labels have the same levels
knn_pred <- factor(knn_pred, levels = levels(test_labels))

# Create a cross table of the predicted and actual values
CrossTable(x = knn_pred, y = test_labels,
           prop.chisq = FALSE, # Do not compute chi-squared test
           prop.t = FALSE, # Do not compute t-test
           prop.r = FALSE, # Do not compute row proportions
           dnn = c('predicted', 'actual')) # Set the names of the dimensions
```


#### Compare the models
```{r}
# Load necessary library
library(ggplot2)

# Reshape the data for plotting
comparison_melt <- reshape2::melt(comparison, id.vars = "Model")

# Create a bar plot
ggplot(comparison_melt, aes(x = Model, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Model", y = "Score", fill = "Metric") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  geom_text(aes(label = round(value, 2)), vjust = -0.3, position = position_dodge(0.9))
```